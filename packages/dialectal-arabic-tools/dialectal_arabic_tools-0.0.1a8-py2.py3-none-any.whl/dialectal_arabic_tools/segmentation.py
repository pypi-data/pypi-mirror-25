# -*- coding: utf8 -*-
import argparse
import codecs
from collections import Counter
from itertools import chain
import numpy as np
from keras.models import load_model
from keras.preprocessing import sequence
from keras.layers import Input, Embedding, Dropout, Bidirectional, LSTM, ChainCRF, Dense, TimeDistributed
import pkg_resources

# http://setuptools.readthedocs.io/en/latest/pkg_resources.html#resourcemanager-api
keras_model = pkg_resources.resource_filename(__name__, "data/seg_keras_model_0922.hdf5")
# input_file = pkg_resources.resource_filename(__name__, "data/example.in.txt")
# output_file = pkg_resources.resource_filename(__name__, "data/example.out.txt")


__author__ = 'disooqi'
__created__ = '21 Sep 2017'

parser = argparse.ArgumentParser(description="Segmentation module for the Dialectal Arabic")
group = parser.add_mutually_exclusive_group()
# file related options
parser.add_argument("-d", "--data-dir", help="directory containing train, test and dev file [default: %(default)s]")
parser.add_argument("-g", "--log-file", dest="log_file", help="log file [default: %(default)s]")
parser.add_argument("-p", "--model-file", dest="model_file",
                    help="directory to save the best models [default: %(default)s]")
parser.add_argument("-r", "--train-set", dest="train_set",
                    help="maximul sentence length (for fixed size input) [default: %(default)s]")  # input size
parser.add_argument("-v", "--dev-set", dest="validation_set",
                    help="source vocabulary size [default: %(default)s]")  # emb matrix row size
parser.add_argument("-s", "--test-set", dest="test_set",
                    help="target vocabulary size [default: %(default)s]")  # emb matrix row size
parser.add_argument('-i', '--input-file', dest='input_file', help='text file to be segmented [default: %(default)s]')
parser.add_argument('-o', '--output-file', dest='output_file',
                    help='the file used to save the result of segmentation [default: %(default)s]')
parser.add_argument('-f', '--format', help='format of the output-file [default: %(default)s]')
# network related
# input
parser.add_argument("-t", "--max-length", dest="maxlen", type=int,
                    help="maximul sentence length (for fixed size input) [default: %(default)s]")  # input size
parser.add_argument("-e", "--emb-size", dest="word_embedding_dim", type=int,
                    help="dimension of embedding [default: %(default)s]")  # emb matrix col size
# learning related
parser.add_argument("-a", "--learning-algorithm", dest="learn_alg",
                    help="optimization algorithm (adam, sgd, adagrad, rmsprop, adadelta) [default: %(default)s]")
parser.add_argument("-b", "--minibatch-size", dest="batch_size", type=int, help="minibatch size [default: %(default)s]")
parser.add_argument("-n", "--epochs", dest="nb_epoch", type=int, help="nb of epochs [default: %(default)s]")
# others
parser.add_argument("-V", "--verbose-level", dest="verbose_level",
                    help="verbosity level (0 < 1 < 2) [default: %(default)s]")
parser.add_argument('-m', '--mode',
                    help='choose between training or testing mode (segmentation) [default: %(default)s]')

parser.set_defaults(
    # file
    data_dir="data/",
    model_file=keras_model,
    # train_set="./data/all_train_f01.txt",
    # validation_set="./data/all_dev_f01.txt",
    # test_set="./data/all_test_f01.txt",
    log_file="run.log",
    # input_file=input_file,
    # output_file=output_file,
    format='conll',
    # network related
    maxlen=50,  # cut texts after this number of words (among top max_features most common words)
    word_embedding_dim=200,

    # learning related
    learn_alg="adam",  # sgd, adagrad, rmsprop, adadelta, adam (default)
    # loss = "binary_crossentropy" # hinge, squared_hinge, binary_crossentropy (default)
    batch_size=10,
    nb_epoch=100,
    verbose_level=1,
    lstm_dim=100,
    max_features=217,  # len(index2word)   and   len(_invert_index(index2word))  ## For evaluation mode only
    nb_pos_tags=4,  # len(index2pos)                                             ## For evaluation mode only
)

args = parser.parse_args()

index2pos = ['S', 'B', 'E', 'M']
pos2index = {'S': 0, 'B': 1, 'E': 2, 'M': 3}
# For evaluation mode only
index2word = ['<PAD>', '<UNK>', 'Ø§', 'Ù„', 'ÙŠ', 'Ùˆ', 'Ù†', 'Ù…', 'Ø¨', 'Øª', 'Ù‡', 'Ø±', 'Ùƒ', 'Ø¹', 'Ø¯', 'Ø­', 'Ù', 'Ø³', 'Ø´', 'Ù‚', 'Ø©', 'Ø¬', 'Ø£', '.', 'Øµ', 'Ø®', 'Ø·', 'Ø²', 'Ù‰', 'Øº', 'a', 'Ø¶', '?', '!', 'Ø¥', 'Ø°', '_', '@', '#', 't', 'e', ':', 'o', 'ØŒ', 'h', 'i', 'Ø«', 'ØŸ', '/', 'l', 'r', 'm', 's', 'n', 'Ø¸', 'Ø¦', 'Ø¡', 'Ø¢', 'd', 'y', 'p', '"', 'c', '1', 'b', '0', ')', '-', 'A', 'S', '2', 'u', 'w', '(', ',', '9', 'k', 'D', 'M', 'f', '5', 'Ø¤', 'z', '7', 'g', '3', 'T', 'H', '8', '4', 'j', 'R', 'Û', 'N', 'B', '6', 'v', 'J', 'L', 'E', '*', '^', 'G', 'Y', 'K', 'F', '~', "'", 'Q', 'V', 'I', 'C', 'W', 'Ú¯', 'O', 'P', 'Ú†', 'ï»»', 'q', 'Z', '>', 'x', 'Ø›', 'U', 'Ú¾', 'â€¹', 'Ù¤', 'â€¦', 'ğŸ˜‚', 'â€', 'â€º', 'ïº', 'ï»´', 'ïº', '=', 'ğŸ˜­', 'Ú©', 'X', 'ï»›', 'ï»§', '<', '[', 'Ú¤', 'ïº®', 'ïº´', 'Ù§', 'ğŸ˜Š', 'ğŸ˜', 'ğŸ˜˜', 'ğŸ˜…', '|', 'ï»·', 'ğŸ™Š', 'â€œ', 'Ù¢', '\\', 'ï»¦', 'Ù¡', 'ïº˜', 'ïºª', 'ï»Ÿ', 'ï»‹', 'ïº¸', 'ïº', 'ïº£', 'ğŸ‰', 'Ù£', 'ğŸ˜', 'â˜º', 'â¤', 'ï¸', 'ğŸ˜œ', 'â€¢', ']', 'Â»', ';', 'ï»¹', 'ïº³', 'ï»¬', 'Ù²', 'ï»£', 'ï»Œ', 'ïº‘', 'ï»Š', 'ï»š', 'ï»ƒ', 'ï»¨', 'ï»¤', 'ï»­', 'ïº¡', 'ïº„', 'ïºŸ', 'ï»®', 'ïº¯', 'ï»', 'ï»¥', 'Ù ', 'Ù¨', 'ğŸ˜‹', 'ğŸŒ¹', 'Ù“', 'ğŸ˜¥', 'Ù«', 'Ù¥', 'â˜¹', 'Ù¦', 'ğŸ™ˆ', 'Ù©', '\ue756', 'ğŸ˜Œ', 'ğŸ˜”', 'â™¥', 'ğŸ‘', 'ğŸ˜†', 'ğŸ˜š', '$', '%']

# For evaluation mode only
word2index = {'<PAD>': 0, '<UNK>': 1, 'Ø§': 2, 'Ù„': 3, 'ÙŠ': 4, 'Ùˆ': 5, 'Ù†': 6, 'Ù…': 7, 'Ø¨': 8, 'Øª': 9, 'Ù‡': 10, 'Ø±': 11, 'Ùƒ': 12, 'Ø¹': 13, 'Ø¯': 14, 'Ø­': 15, 'Ù': 16, 'Ø³': 17, 'Ø´': 18, 'Ù‚': 19, 'Ø©': 20, 'Ø¬': 21, 'Ø£': 22, '.': 23, 'Øµ': 24, 'Ø®': 25, 'Ø·': 26, 'Ø²': 27, 'Ù‰': 28, 'Øº': 29, 'a': 30, 'Ø¶': 31, '?': 32, '!': 33, 'Ø¥': 34, 'Ø°': 35, '_': 36, '@': 37, '#': 38, 't': 39, 'e': 40, ':': 41, 'o': 42, 'ØŒ': 43, 'h': 44, 'i': 45, 'Ø«': 46, 'ØŸ': 47, '/': 48, 'l': 49, 'r': 50, 'm': 51, 's': 52, 'n': 53, 'Ø¸': 54, 'Ø¦': 55, 'Ø¡': 56, 'Ø¢': 57, 'd': 58, 'y': 59, 'p': 60, '"': 61, 'c': 62, '1': 63, 'b': 64, '0': 65, ')': 66, '-': 67, 'A': 68, 'S': 69, '2': 70, 'u': 71, 'w': 72, '(': 73, ',': 74, '9': 75, 'k': 76, 'D': 77, 'M': 78, 'f': 79, '5': 80, 'Ø¤': 81, 'z': 82, '7': 83, 'g': 84, '3': 85, 'T': 86, 'H': 87, '8': 88, '4': 89, 'j': 90, 'R': 91, 'Û': 92, 'N': 93, 'B': 94, '6': 95, 'v': 96, 'J': 97, 'L': 98, 'E': 99, '*': 100, '^': 101, 'G': 102, 'Y': 103, 'K': 104, 'F': 105, '~': 106, "'": 107, 'Q': 108, 'V': 109, 'I': 110, 'C': 111, 'W': 112, 'Ú¯': 113, 'O': 114, 'P': 115, 'Ú†': 116, 'ï»»': 117, 'q': 118, 'Z': 119, '>': 120, 'x': 121, 'Ø›': 122, 'U': 123, 'Ú¾': 124, 'â€¹': 125, 'Ù¤': 126, 'â€¦': 127, 'ğŸ˜‚': 128, 'â€': 129, 'â€º': 130, 'ïº': 131, 'ï»´': 132, 'ïº': 133, '=': 134, 'ğŸ˜­': 135, 'Ú©': 136, 'X': 137, 'ï»›': 138, 'ï»§': 139, '<': 140, '[': 141, 'Ú¤': 142, 'ïº®': 143, 'ïº´': 144, 'Ù§': 145, 'ğŸ˜Š': 146, 'ğŸ˜': 147, 'ğŸ˜˜': 148, 'ğŸ˜…': 149, '|': 150, 'ï»·': 151, 'ğŸ™Š': 152, 'â€œ': 153, 'Ù¢': 154, '\\': 155, 'ï»¦': 156, 'Ù¡': 157, 'ïº˜': 158, 'ïºª': 159, 'ï»Ÿ': 160, 'ï»‹': 161, 'ïº¸': 162, 'ïº': 163, 'ïº£': 164, 'ğŸ‰': 165, 'Ù£': 166, 'ğŸ˜': 167, 'â˜º': 168, 'â¤': 169, 'ï¸': 170, 'ğŸ˜œ': 171, 'â€¢': 172, ']': 173, 'Â»': 174, ';': 175, 'ï»¹': 176, 'ïº³': 177, 'ï»¬': 178, 'Ù²': 179, 'ï»£': 180, 'ï»Œ': 181, 'ïº‘': 182, 'ï»Š': 183, 'ï»š': 184, 'ï»ƒ': 185, 'ï»¨': 186, 'ï»¤': 187, 'ï»­': 188, 'ïº¡': 189, 'ïº„': 190, 'ïºŸ': 191, 'ï»®': 192, 'ïº¯': 193, 'ï»': 194, 'ï»¥': 195, 'Ù ': 196, 'Ù¨': 197, 'ğŸ˜‹': 198, 'ğŸŒ¹': 199, 'Ù“': 200, 'ğŸ˜¥': 201, 'Ù«': 202, 'Ù¥': 203, 'â˜¹': 204, 'Ù¦': 205, 'ğŸ™ˆ': 206, 'Ù©': 207, '\ue756': 208, 'ğŸ˜Œ': 209, 'ğŸ˜”': 210, 'â™¥': 211, 'ğŸ‘': 212, 'ğŸ˜†': 213, 'ğŸ˜š': 214, '$': 215, '%': 216}

def remove_diacrtics(token):
    # TODO implement 'remove_diacrtics'
    return token

def _fit_term_index(terms, reserved=[], preprocess=lambda x: x):
    all_terms = chain(*terms)
    all_terms = map(preprocess, all_terms)
    term_freqs = Counter(all_terms).most_common()
    id2term = reserved + [term for term, tf in term_freqs]
    return id2term


def _invert_index(id2term):
    return {term: i for i, term in enumerate(id2term)}


def load_file_as_words(file_path):
    '''
    :param file_path: path of a fule contains a conll format of segmentation
    :return: a tuple of two lists of lists of characters and tags
    '''
    with codecs.open(file_path, encoding='utf-8') as conell:
        list_of_lines = [line.strip().split() for line in conell if len(line.strip().split()) == 2]
        chars, targets = list(zip(*list_of_lines))
        words = ''.join(chars).split('WB')
        target_of_words = ''.join(targets).split('WB')

        words = [tuple(word) for word in words]
        target_of_words = [tuple(trg) for trg in target_of_words]

        return words, target_of_words

def load_txt_as_ch_list(file_path):
    words = list()
    with codecs.open(file_path, encoding='utf-8') as plan_txt:
        for line in plan_txt:
            tokens = line.strip().split()
            for token in tokens:
                words.append(list(token))

    return words


def build_embeddings(mxl):
    # Add UNKNOWN and
    targets = np.array([1 for i in range(0, mxl)])
    one_hot_targets = np.eye(args.word_embedding_dim)[targets]
    return one_hot_targets


def segment_file(infile, outfile, dl_model=keras_model):
    embeddings = build_embeddings(args.max_features)
    #
    print('Loading data...')
    X_chars = load_txt_as_ch_list(infile)
    X_idxs = np.array([[word2index.get(w, word2index['<UNK>']) for w in words] for words in X_chars])
    X_idxs_padded = sequence.pad_sequences(X_idxs, maxlen=args.maxlen, padding='post')

    print('loading model...')
    word_input = Input(shape=(args.maxlen,), dtype='int32', name='word_input')
    word_emb = Embedding(embeddings.shape[0], args.word_embedding_dim, input_length=args.maxlen, name='word_emb',
                         weights=[embeddings])(word_input)
    word_emb_d = Dropout(0.5)(word_emb)
    bilstm = Bidirectional(LSTM(args.lstm_dim, return_sequences=True))(word_emb_d)
    bilstm_d = Dropout(0.5)(bilstm)
    dense = TimeDistributed(Dense(args.nb_pos_tags))(bilstm_d)
    crf = ChainCRF()
    crf_output = crf(dense)
    model = load_model(dl_model,
                       custom_objects={'ChainCRF': ChainCRF, 'sparse_loss': crf.sparse_loss}, compile=False)
    model.compile(loss=crf.sparse_loss, optimizer='adam', metrics=['sparse_categorical_accuracy'])

    prediction = model.predict(X_idxs_padded, args.batch_size, verbose=0)

    # TODO - 01. the function should return a segmented string
    with codecs.open(outfile, mode='w', encoding='utf-8') as results:
        for pred, word in zip(np.argmax(prediction, axis=2), X_chars):
            assert len(pred) >= len(word)

            for ch, est in zip(word, pred):
                results.write(ch + '\t' + index2pos[est] + '\n')
            else:
                results.write('WB\tWB\n')


if '__name__' == '__main__':
    pass
