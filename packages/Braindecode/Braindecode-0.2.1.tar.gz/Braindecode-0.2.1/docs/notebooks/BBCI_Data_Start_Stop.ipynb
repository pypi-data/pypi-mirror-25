{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Decode BBCI Data with Start-Stop-Markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to read and decode BBCI data with start and stop markers. The data loading part is more complicated and it is advised to read the other tutorials before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup logging to see outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.DEBUG, stream=sys.stdout)\n",
    "log = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit more complicated than before since we have to add breaks etc. Here I now opt to add breaks do all preprocessings per run and only later combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.datautil.splitters import concatenate_sets\n",
    "from braindecode.datautil.trial_segment import create_signal_target_from_raw_mne, add_breaks\n",
    "from braindecode.datasets.bbci import load_bbci_sets_from_folder\n",
    "from copy import deepcopy\n",
    "from braindecode.mne_ext.signalproc import resample_cnt, mne_apply\n",
    "from braindecode.datautil.signalproc import lowpass_cnt\n",
    "from braindecode.datautil.signalproc import exponential_running_standardize\n",
    "\n",
    "def create_cnts(folder, runs, name_to_start_code, name_to_stop_code, break_start_offset_ms,\n",
    "              break_stop_offset_ms, break_start_code, break_stop_code):\n",
    "    # Load data\n",
    "    cnts = load_bbci_sets_from_folder(folder, runs)\n",
    "    \n",
    "    # Now do some preprocessings:\n",
    "    # Adding breaks, resampling to 250 Hz, lowpass below 38, eponential standardization\n",
    "    break_start_code = -1\n",
    "    break_stop_code = -2\n",
    "    \n",
    "    new_cnts = []\n",
    "    for cnt in cnts:\n",
    "        # Only take some channels \n",
    "        #cnt = cnt.drop_channels(['STI 014']) # This would remove stimulus channel\n",
    "        cnt = cnt.pick_channels(['C3', 'CPz' 'C4'])\n",
    "        # add breaks\n",
    "        new_events = add_breaks(\n",
    "            cnt.info['events'], cnt.info['sfreq'],\n",
    "            break_start_code=break_start_code, break_stop_code=break_stop_code,\n",
    "            name_to_start_codes=name_to_start_code, name_to_stop_codes=name_to_stop_code,\n",
    "            min_break_length_ms=5000, max_break_length_ms=9000)\n",
    "        n_break_start_offset = int(cnt.info['sfreq'] * break_start_offset_ms / 1000.0)\n",
    "        n_break_stop_offset = int(cnt.info['sfreq'] * break_stop_offset_ms / 1000.0)\n",
    "        # lets add some offset to break start and stop\n",
    "        # new_events[:,2] contains event codes, new_events[:,0] the sample indices\n",
    "        # new_events[:,1] is always 0 for my loading of BBCI data\n",
    "        new_events[new_events[:,2] == break_start_code, 0] += n_break_start_offset\n",
    "        # 0.5 sec for break stop\n",
    "        new_events[new_events[:,2] == break_stop_code, 0] +=  n_break_stop_offset\n",
    "        cnt.info['events'] = new_events\n",
    "        log.info(\"Preprocessing....\")\n",
    "        cnt = mne_apply(lambda a: lowpass_cnt(a, 38,cnt.info['sfreq'], axis=1), cnt)\n",
    "        cnt = resample_cnt(cnt, 250)\n",
    "        # mne apply will apply the function to the data (a 2d-numpy-array)\n",
    "        # have to transpose data back and forth, since\n",
    "        # exponential_running_standardize expects time x chans order\n",
    "        # while mne object has chans x time order\n",
    "        cnt = mne_apply(lambda a: exponential_running_standardize(\n",
    "            a.T, init_block_size=1000,factor_new=0.001, eps=1e-4).T,\n",
    "            cnt)\n",
    "        new_cnts.append(cnt)\n",
    "    return new_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-04 17:17:37,895 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R01-8/AnLaNBD1S001R01_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=151350\n",
      "    Range : 0 ... 151349 =      0.000 ...   605.396 secs\n",
      "Ready.\n",
      "2017-09-04 17:17:38,880 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R01-8/AnLaNBD1S001R02_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=153500\n",
      "    Range : 0 ... 153499 =      0.000 ...   613.996 secs\n",
      "Ready.\n",
      "2017-09-04 17:17:39,855 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R01-8/AnLaNBD1S001R03_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=180700\n",
      "    Range : 0 ... 180699 =      0.000 ...   722.796 secs\n",
      "Ready.\n",
      "2017-09-04 17:17:41,062 INFO : Preprocessing....\n",
      "2017-09-04 17:17:41,070 INFO : Just copying data, no resampling, since new sampling rate same.\n",
      "2017-09-04 17:17:41,166 INFO : Preprocessing....\n",
      "2017-09-04 17:17:41,171 INFO : Just copying data, no resampling, since new sampling rate same.\n",
      "2017-09-04 17:17:41,277 INFO : Preprocessing....\n",
      "2017-09-04 17:17:41,282 INFO : Just copying data, no resampling, since new sampling rate same.\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "name_to_start_code = OrderedDict([('Right Hand', 1), ('Feet', 4),\n",
    "            ('Rotation', 8), ('Words', [10])])\n",
    "\n",
    "name_to_stop_code = OrderedDict([('Right Hand', [20,21,22,23,24,28,30]),\n",
    "        ('Feet', [20,21,22,23,24,28,30]),\n",
    "        ('Rotation', [20,21,22,23,24,28,30]), \n",
    "        ('Words', [20,21,22,23,24,28,30])])\n",
    "\n",
    "break_start_offset_ms = 1000\n",
    "break_stop_offset_ms = -500\n",
    "# pick some numbers that were not used before/do not exist in markers\n",
    "break_start_code = -1\n",
    "break_stop_code = -2\n",
    "train_runs = [1,2,3]\n",
    "train_cnts = create_cnts('/home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R01-8/', \n",
    "                         train_runs,\n",
    "                         name_to_start_code,\n",
    "                         name_to_stop_code, break_start_offset_ms,\n",
    "                         break_stop_offset_ms, break_start_code, break_stop_code)\n",
    "\n",
    "name_to_code_with_breaks = deepcopy(name_to_start_code)\n",
    "name_to_code_with_breaks['Break'] = break_start_code\n",
    "name_to_stop_code_with_breaks = deepcopy(name_to_stop_code)\n",
    "name_to_stop_code_with_breaks['Break'] = break_stop_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-04 17:17:41,349 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R09-10/AnLaNBD1S001R09_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=152050\n",
      "    Range : 0 ... 152049 =      0.000 ...   608.196 secs\n",
      "Ready.\n",
      "2017-09-04 17:17:42,321 INFO : Loading /home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R09-10/AnLaNBD1S001R10_1-1_250Hz.BBCI.mat\n",
      "Creating RawArray with float64 data, n_channels=64, n_times=151100\n",
      "    Range : 0 ... 151099 =      0.000 ...   604.396 secs\n",
      "Ready.\n",
      "2017-09-04 17:17:43,336 INFO : Preprocessing....\n",
      "2017-09-04 17:17:43,340 INFO : Just copying data, no resampling, since new sampling rate same.\n",
      "2017-09-04 17:17:43,436 INFO : Preprocessing....\n",
      "2017-09-04 17:17:43,440 INFO : Just copying data, no resampling, since new sampling rate same.\n"
     ]
    }
   ],
   "source": [
    "test_runs = [9,10]\n",
    "test_cnts = create_cnts('/home/schirrmr/data/robot-hall/AnLa/AnLaNBD1R09-10/', test_runs, name_to_start_code,\n",
    "           name_to_stop_code, break_start_offset_ms,\n",
    "              break_stop_offset_ms, break_start_code, break_stop_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already create the model now, since we need to know the receptive field size for properly cutting out the data to predict. We need to cut out data starting at -receptive_field_size samples before the first sample we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 predictions per input/trial\n",
      "Receptive field: 518/2072.00 (samples/ms)\n"
     ]
    }
   ],
   "source": [
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "\n",
    "# Set if you want to use GPU\n",
    "# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.\n",
    "cuda = True\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "\n",
    "# This will determine how many crops are processed in parallel\n",
    "input_time_length = 650\n",
    "in_chans = train_cnts[0].get_data().shape[0]\n",
    "# final_conv_length determines the size of the receptive field of the ConvNet\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=5, input_time_length=input_time_length,\n",
    "                        final_conv_length=29).create_network()\n",
    "to_dense_prediction_model(model)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "from braindecode.torch_ext.util import np_to_var\n",
    "import numpy as np\n",
    "# determine output size\n",
    "test_input = np_to_var(np.ones((2, in_chans, input_time_length, 1), dtype=np.float32))\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "out = model(test_input)\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]\n",
    "print(\"{:d} predictions per input/trial\".format(n_preds_per_input))\n",
    "n_receptive_field = input_time_length - n_preds_per_input\n",
    "receptive_field_ms = n_receptive_field * 1000.0 / train_cnts[0].info['sfreq']\n",
    "print(\"Receptive field: {:d}/{:.2f} (samples/ms)\".format(n_receptive_field,\n",
    "                                                      receptive_field_ms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SignalAndTarget Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-04 17:17:47,054 WARNING : No end marker for start marker code 1 at sample 150051 found.\n",
      "2017-09-04 17:17:47,054 INFO : Trial per class:\n",
      "Counter({'Right Hand': 29, 'Words': 21, 'Feet': 19, 'Break': 18, 'Rotation': 4})\n",
      "2017-09-04 17:17:47,059 WARNING : No end marker for start marker code 4 at sample 152701 found.\n",
      "2017-09-04 17:17:47,060 INFO : Trial per class:\n",
      "Counter({'Feet': 31, 'Words': 26, 'Break': 20, 'Right Hand': 18, 'Rotation': 6})\n",
      "2017-09-04 17:17:47,064 WARNING : No end marker for start marker code 1 at sample 179301 found.\n",
      "2017-09-04 17:17:47,065 INFO : Trial per class:\n",
      "Counter({'Feet': 38, 'Words': 29, 'Break': 23, 'Right Hand': 22, 'Rotation': 7})\n"
     ]
    }
   ],
   "source": [
    "train_sets = [create_signal_target_from_raw_mne(cnt, name_to_code_with_breaks, [-receptive_field_ms,0], \n",
    "                                         name_to_stop_code_with_breaks) for cnt in train_cnts]\n",
    "train_set = concatenate_sets(train_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-04 17:17:47,090 WARNING : No end marker for start marker code 4 at sample 150601 found.\n",
      "2017-09-04 17:17:47,091 INFO : Trial per class:\n",
      "Counter({'Feet': 24, 'Right Hand': 24, 'Words': 19, 'Break': 19, 'Rotation': 10})\n",
      "2017-09-04 17:17:47,095 WARNING : No end marker for start marker code 4 at sample 149601 found.\n",
      "2017-09-04 17:17:47,096 INFO : Trial per class:\n",
      "Counter({'Feet': 30, 'Right Hand': 22, 'Words': 21, 'Break': 20, 'Rotation': 8})\n"
     ]
    }
   ],
   "source": [
    "test_sets = [create_signal_target_from_raw_mne(cnt, name_to_code_with_breaks, [-receptive_field_ms,0], \n",
    "                                         name_to_stop_code_with_breaks) for cnt in test_cnts]\n",
    "test_set = concatenate_sets(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.datautil.splitters import split_into_two_sets\n",
    "\n",
    "train_set, valid_set = split_into_two_sets(train_set, first_set_fraction=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup optimizer and iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "iterator = CropsFromTrialsIterator(batch_size=32,input_time_length=input_time_length,\n",
    "                                  n_preds_per_input=n_preds_per_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Monitors, Loss function, Stop Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.experiments.monitors import RuntimeMonitor, LossMonitor, CroppedTrialMisclassMonitor, MisclassMonitor\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs\n",
    "import torch.nn.functional as F\n",
    "import torch as th\n",
    "from braindecode.torch_ext.modules import Expression\n",
    "\n",
    "\n",
    "loss_function = lambda preds, targets: F.nll_loss(th.mean(preds, dim=2, keepdim=False), targets)\n",
    "\n",
    "model_constraint = None\n",
    "monitors = [LossMonitor(), MisclassMonitor(col_suffix='sample_misclass'),\n",
    "            CroppedTrialMisclassMonitor(input_time_length), RuntimeMonitor(),]\n",
    "stop_criterion = MaxEpochs(20)\n",
    "exp = Experiment(model, train_set, valid_set, test_set, iterator, loss_function, optimizer, model_constraint,\n",
    "          monitors, stop_criterion, remember_best_column='valid_misclass',\n",
    "          run_after_early_stop=True, batch_modifier=None, cuda=cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-04 17:17:47,219 INFO : Run until first stop...\n",
      "2017-09-04 17:17:47,865 INFO : Epoch 0\n",
      "2017-09-04 17:17:47,866 INFO : train_loss                11.53333\n",
      "2017-09-04 17:17:47,867 INFO : valid_loss                12.37084\n",
      "2017-09-04 17:17:47,868 INFO : test_loss                 11.33717\n",
      "2017-09-04 17:17:47,869 INFO : train_sample_misclass     0.93548\n",
      "2017-09-04 17:17:47,869 INFO : valid_sample_misclass     0.98502\n",
      "2017-09-04 17:17:47,870 INFO : test_sample_misclass      0.91691\n",
      "2017-09-04 17:17:47,871 INFO : train_misclass            0.93976\n",
      "2017-09-04 17:17:47,871 INFO : valid_misclass            0.96774\n",
      "2017-09-04 17:17:47,872 INFO : test_misclass             0.90863\n",
      "2017-09-04 17:17:47,873 INFO : runtime                   0.00000\n",
      "2017-09-04 17:17:47,874 INFO : \n",
      "2017-09-04 17:17:47,875 INFO : New best valid_misclass: 0.967742\n",
      "2017-09-04 17:17:47,876 INFO : \n",
      "2017-09-04 17:17:48,405 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:17:49,011 INFO : Epoch 1\n",
      "2017-09-04 17:17:49,012 INFO : train_loss                1.50393\n",
      "2017-09-04 17:17:49,013 INFO : valid_loss                1.45453\n",
      "2017-09-04 17:17:49,014 INFO : test_loss                 1.55869\n",
      "2017-09-04 17:17:49,014 INFO : train_sample_misclass     0.64498\n",
      "2017-09-04 17:17:49,015 INFO : valid_sample_misclass     0.62718\n",
      "2017-09-04 17:17:49,016 INFO : test_sample_misclass      0.66700\n",
      "2017-09-04 17:17:49,017 INFO : train_misclass            0.58233\n",
      "2017-09-04 17:17:49,017 INFO : valid_misclass            0.58065\n",
      "2017-09-04 17:17:49,018 INFO : test_misclass             0.62437\n",
      "2017-09-04 17:17:49,019 INFO : runtime                   1.18624\n",
      "2017-09-04 17:17:49,019 INFO : \n",
      "2017-09-04 17:17:49,021 INFO : New best valid_misclass: 0.580645\n",
      "2017-09-04 17:17:49,022 INFO : \n",
      "2017-09-04 17:17:49,545 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:17:50,151 INFO : Epoch 2\n",
      "2017-09-04 17:17:50,152 INFO : train_loss                1.41951\n",
      "2017-09-04 17:17:50,153 INFO : valid_loss                1.48343\n",
      "2017-09-04 17:17:50,154 INFO : test_loss                 1.53018\n",
      "2017-09-04 17:17:50,155 INFO : train_sample_misclass     0.60707\n",
      "2017-09-04 17:17:50,155 INFO : valid_sample_misclass     0.62290\n",
      "2017-09-04 17:17:50,156 INFO : test_sample_misclass      0.64429\n",
      "2017-09-04 17:17:50,157 INFO : train_misclass            0.56627\n",
      "2017-09-04 17:17:50,157 INFO : valid_misclass            0.62903\n",
      "2017-09-04 17:17:50,158 INFO : test_misclass             0.64975\n",
      "2017-09-04 17:17:50,159 INFO : runtime                   1.13945\n",
      "2017-09-04 17:17:50,160 INFO : \n",
      "2017-09-04 17:17:50,684 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:17:51,291 INFO : Epoch 3\n",
      "2017-09-04 17:17:51,292 INFO : train_loss                1.42581\n",
      "2017-09-04 17:17:51,293 INFO : valid_loss                1.49817\n",
      "2017-09-04 17:17:51,294 INFO : test_loss                 1.61901\n",
      "2017-09-04 17:17:51,294 INFO : train_sample_misclass     0.61668\n",
      "2017-09-04 17:17:51,295 INFO : valid_sample_misclass     0.65217\n",
      "2017-09-04 17:17:51,296 INFO : test_sample_misclass      0.67644\n",
      "2017-09-04 17:17:51,297 INFO : train_misclass            0.60241\n",
      "2017-09-04 17:17:51,297 INFO : valid_misclass            0.72581\n",
      "2017-09-04 17:17:51,298 INFO : test_misclass             0.67513\n",
      "2017-09-04 17:17:51,299 INFO : runtime                   1.13913\n",
      "2017-09-04 17:17:51,299 INFO : \n",
      "2017-09-04 17:17:51,823 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:17:52,429 INFO : Epoch 4\n",
      "2017-09-04 17:17:52,430 INFO : train_loss                1.38586\n",
      "2017-09-04 17:17:52,431 INFO : valid_loss                1.48687\n",
      "2017-09-04 17:17:52,432 INFO : test_loss                 1.55094\n",
      "2017-09-04 17:17:52,433 INFO : train_sample_misclass     0.59819\n",
      "2017-09-04 17:17:52,433 INFO : valid_sample_misclass     0.64463\n",
      "2017-09-04 17:17:52,434 INFO : test_sample_misclass      0.65671\n",
      "2017-09-04 17:17:52,435 INFO : train_misclass            0.52610\n",
      "2017-09-04 17:17:52,435 INFO : valid_misclass            0.67742\n",
      "2017-09-04 17:17:52,436 INFO : test_misclass             0.62944\n",
      "2017-09-04 17:17:52,437 INFO : runtime                   1.13857\n",
      "2017-09-04 17:17:52,438 INFO : \n",
      "2017-09-04 17:17:52,961 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:17:53,569 INFO : Epoch 5\n",
      "2017-09-04 17:17:53,570 INFO : train_loss                1.41485\n",
      "2017-09-04 17:17:53,570 INFO : valid_loss                1.44874\n",
      "2017-09-04 17:17:53,571 INFO : test_loss                 1.55589\n",
      "2017-09-04 17:17:53,572 INFO : train_sample_misclass     0.60224\n",
      "2017-09-04 17:17:53,573 INFO : valid_sample_misclass     0.62897\n",
      "2017-09-04 17:17:53,573 INFO : test_sample_misclass      0.65083\n",
      "2017-09-04 17:17:53,574 INFO : train_misclass            0.51406\n",
      "2017-09-04 17:17:53,575 INFO : valid_misclass            0.58065\n",
      "2017-09-04 17:17:53,575 INFO : test_misclass             0.63452\n",
      "2017-09-04 17:17:53,576 INFO : runtime                   1.13840\n",
      "2017-09-04 17:17:53,577 INFO : \n",
      "2017-09-04 17:17:53,579 INFO : New best valid_misclass: 0.580645\n",
      "2017-09-04 17:17:53,580 INFO : \n",
      "2017-09-04 17:17:54,102 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:17:54,708 INFO : Epoch 6\n",
      "2017-09-04 17:17:54,709 INFO : train_loss                1.36745\n",
      "2017-09-04 17:17:54,710 INFO : valid_loss                1.41226\n",
      "2017-09-04 17:17:54,711 INFO : test_loss                 1.52581\n",
      "2017-09-04 17:17:54,712 INFO : train_sample_misclass     0.58987\n",
      "2017-09-04 17:17:54,712 INFO : valid_sample_misclass     0.63906\n",
      "2017-09-04 17:17:54,713 INFO : test_sample_misclass      0.64640\n",
      "2017-09-04 17:17:54,714 INFO : train_misclass            0.49398\n",
      "2017-09-04 17:17:54,714 INFO : valid_misclass            0.61290\n",
      "2017-09-04 17:17:54,715 INFO : test_misclass             0.60406\n",
      "2017-09-04 17:17:54,716 INFO : runtime                   1.14130\n",
      "2017-09-04 17:17:54,717 INFO : \n",
      "2017-09-04 17:17:55,240 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:17:55,847 INFO : Epoch 7\n",
      "2017-09-04 17:17:55,848 INFO : train_loss                1.37782\n",
      "2017-09-04 17:17:55,849 INFO : valid_loss                1.43321\n",
      "2017-09-04 17:17:55,850 INFO : test_loss                 1.50327\n",
      "2017-09-04 17:17:55,851 INFO : train_sample_misclass     0.57490\n",
      "2017-09-04 17:17:55,852 INFO : valid_sample_misclass     0.62826\n",
      "2017-09-04 17:17:55,852 INFO : test_sample_misclass      0.63315\n",
      "2017-09-04 17:17:55,853 INFO : train_misclass            0.51406\n",
      "2017-09-04 17:17:55,854 INFO : valid_misclass            0.70968\n",
      "2017-09-04 17:17:55,854 INFO : test_misclass             0.63452\n",
      "2017-09-04 17:17:55,855 INFO : runtime                   1.13720\n",
      "2017-09-04 17:17:55,856 INFO : \n",
      "2017-09-04 17:17:56,380 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:17:56,989 INFO : Epoch 8\n",
      "2017-09-04 17:17:56,990 INFO : train_loss                1.37211\n",
      "2017-09-04 17:17:56,991 INFO : valid_loss                1.42880\n",
      "2017-09-04 17:17:56,992 INFO : test_loss                 1.53159\n",
      "2017-09-04 17:17:56,993 INFO : train_sample_misclass     0.58581\n",
      "2017-09-04 17:17:56,993 INFO : valid_sample_misclass     0.65533\n",
      "2017-09-04 17:17:56,994 INFO : test_sample_misclass      0.67241\n",
      "2017-09-04 17:17:56,995 INFO : train_misclass            0.56225\n",
      "2017-09-04 17:17:56,996 INFO : valid_misclass            0.67742\n",
      "2017-09-04 17:17:56,996 INFO : test_misclass             0.66497\n",
      "2017-09-04 17:17:56,997 INFO : runtime                   1.14047\n",
      "2017-09-04 17:17:56,998 INFO : \n",
      "2017-09-04 17:17:57,520 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:17:58,127 INFO : Epoch 9\n",
      "2017-09-04 17:17:58,128 INFO : train_loss                1.36910\n",
      "2017-09-04 17:17:58,128 INFO : valid_loss                1.43300\n",
      "2017-09-04 17:17:58,129 INFO : test_loss                 1.54117\n",
      "2017-09-04 17:17:58,130 INFO : train_sample_misclass     0.59271\n",
      "2017-09-04 17:17:58,131 INFO : valid_sample_misclass     0.65435\n",
      "2017-09-04 17:17:58,131 INFO : test_sample_misclass      0.67089\n",
      "2017-09-04 17:17:58,132 INFO : train_misclass            0.57831\n",
      "2017-09-04 17:17:58,133 INFO : valid_misclass            0.67742\n",
      "2017-09-04 17:17:58,134 INFO : test_misclass             0.66497\n",
      "2017-09-04 17:17:58,134 INFO : runtime                   1.14038\n",
      "2017-09-04 17:17:58,135 INFO : \n",
      "2017-09-04 17:17:58,659 INFO : Time only for training updates: 0.40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-04 17:17:59,268 INFO : Epoch 10\n",
      "2017-09-04 17:17:59,270 INFO : train_loss                1.34061\n",
      "2017-09-04 17:17:59,270 INFO : valid_loss                1.47586\n",
      "2017-09-04 17:17:59,271 INFO : test_loss                 1.52879\n",
      "2017-09-04 17:17:59,272 INFO : train_sample_misclass     0.57797\n",
      "2017-09-04 17:17:59,273 INFO : valid_sample_misclass     0.64290\n",
      "2017-09-04 17:17:59,273 INFO : test_sample_misclass      0.65074\n",
      "2017-09-04 17:17:59,274 INFO : train_misclass            0.48594\n",
      "2017-09-04 17:17:59,275 INFO : valid_misclass            0.56452\n",
      "2017-09-04 17:17:59,275 INFO : test_misclass             0.60406\n",
      "2017-09-04 17:17:59,276 INFO : runtime                   1.13859\n",
      "2017-09-04 17:17:59,277 INFO : \n",
      "2017-09-04 17:17:59,279 INFO : New best valid_misclass: 0.564516\n",
      "2017-09-04 17:17:59,279 INFO : \n",
      "2017-09-04 17:17:59,809 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:18:00,417 INFO : Epoch 11\n",
      "2017-09-04 17:18:00,418 INFO : train_loss                1.37122\n",
      "2017-09-04 17:18:00,418 INFO : valid_loss                1.39534\n",
      "2017-09-04 17:18:00,419 INFO : test_loss                 1.57000\n",
      "2017-09-04 17:18:00,420 INFO : train_sample_misclass     0.58240\n",
      "2017-09-04 17:18:00,420 INFO : valid_sample_misclass     0.64185\n",
      "2017-09-04 17:18:00,421 INFO : test_sample_misclass      0.67935\n",
      "2017-09-04 17:18:00,422 INFO : train_misclass            0.51004\n",
      "2017-09-04 17:18:00,423 INFO : valid_misclass            0.64516\n",
      "2017-09-04 17:18:00,423 INFO : test_misclass             0.64467\n",
      "2017-09-04 17:18:00,424 INFO : runtime                   1.15042\n",
      "2017-09-04 17:18:00,425 INFO : \n",
      "2017-09-04 17:18:00,955 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:18:01,563 INFO : Epoch 12\n",
      "2017-09-04 17:18:01,564 INFO : train_loss                1.38608\n",
      "2017-09-04 17:18:01,565 INFO : valid_loss                1.48440\n",
      "2017-09-04 17:18:01,565 INFO : test_loss                 1.57658\n",
      "2017-09-04 17:18:01,566 INFO : train_sample_misclass     0.60330\n",
      "2017-09-04 17:18:01,567 INFO : valid_sample_misclass     0.62552\n",
      "2017-09-04 17:18:01,568 INFO : test_sample_misclass      0.66465\n",
      "2017-09-04 17:18:01,568 INFO : train_misclass            0.56225\n",
      "2017-09-04 17:18:01,569 INFO : valid_misclass            0.61290\n",
      "2017-09-04 17:18:01,570 INFO : test_misclass             0.66497\n",
      "2017-09-04 17:18:01,570 INFO : runtime                   1.14569\n",
      "2017-09-04 17:18:01,571 INFO : \n",
      "2017-09-04 17:18:02,101 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:18:02,709 INFO : Epoch 13\n",
      "2017-09-04 17:18:02,710 INFO : train_loss                1.34758\n",
      "2017-09-04 17:18:02,711 INFO : valid_loss                1.47008\n",
      "2017-09-04 17:18:02,712 INFO : test_loss                 1.57330\n",
      "2017-09-04 17:18:02,713 INFO : train_sample_misclass     0.56943\n",
      "2017-09-04 17:18:02,713 INFO : valid_sample_misclass     0.63394\n",
      "2017-09-04 17:18:02,714 INFO : test_sample_misclass      0.65246\n",
      "2017-09-04 17:18:02,715 INFO : train_misclass            0.50201\n",
      "2017-09-04 17:18:02,715 INFO : valid_misclass            0.56452\n",
      "2017-09-04 17:18:02,716 INFO : test_misclass             0.58883\n",
      "2017-09-04 17:18:02,717 INFO : runtime                   1.14636\n",
      "2017-09-04 17:18:02,718 INFO : \n",
      "2017-09-04 17:18:02,719 INFO : New best valid_misclass: 0.564516\n",
      "2017-09-04 17:18:02,720 INFO : \n",
      "2017-09-04 17:18:03,248 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:18:03,857 INFO : Epoch 14\n",
      "2017-09-04 17:18:03,858 INFO : train_loss                1.35378\n",
      "2017-09-04 17:18:03,859 INFO : valid_loss                1.45730\n",
      "2017-09-04 17:18:03,860 INFO : test_loss                 1.53986\n",
      "2017-09-04 17:18:03,861 INFO : train_sample_misclass     0.58917\n",
      "2017-09-04 17:18:03,861 INFO : valid_sample_misclass     0.63594\n",
      "2017-09-04 17:18:03,862 INFO : test_sample_misclass      0.65305\n",
      "2017-09-04 17:18:03,863 INFO : train_misclass            0.48594\n",
      "2017-09-04 17:18:03,864 INFO : valid_misclass            0.59677\n",
      "2017-09-04 17:18:03,864 INFO : test_misclass             0.63452\n",
      "2017-09-04 17:18:03,865 INFO : runtime                   1.14647\n",
      "2017-09-04 17:18:03,866 INFO : \n",
      "2017-09-04 17:18:04,395 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:18:05,004 INFO : Epoch 15\n",
      "2017-09-04 17:18:05,005 INFO : train_loss                1.32959\n",
      "2017-09-04 17:18:05,006 INFO : valid_loss                1.39044\n",
      "2017-09-04 17:18:05,007 INFO : test_loss                 1.51507\n",
      "2017-09-04 17:18:05,007 INFO : train_sample_misclass     0.56187\n",
      "2017-09-04 17:18:05,008 INFO : valid_sample_misclass     0.62517\n",
      "2017-09-04 17:18:05,009 INFO : test_sample_misclass      0.64205\n",
      "2017-09-04 17:18:05,010 INFO : train_misclass            0.46586\n",
      "2017-09-04 17:18:05,010 INFO : valid_misclass            0.56452\n",
      "2017-09-04 17:18:05,011 INFO : test_misclass             0.61929\n",
      "2017-09-04 17:18:05,012 INFO : runtime                   1.14746\n",
      "2017-09-04 17:18:05,012 INFO : \n",
      "2017-09-04 17:18:05,014 INFO : New best valid_misclass: 0.564516\n",
      "2017-09-04 17:18:05,015 INFO : \n",
      "2017-09-04 17:18:05,545 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:18:06,153 INFO : Epoch 16\n",
      "2017-09-04 17:18:06,154 INFO : train_loss                1.35525\n",
      "2017-09-04 17:18:06,155 INFO : valid_loss                1.47013\n",
      "2017-09-04 17:18:06,156 INFO : test_loss                 1.55756\n",
      "2017-09-04 17:18:06,156 INFO : train_sample_misclass     0.58248\n",
      "2017-09-04 17:18:06,157 INFO : valid_sample_misclass     0.61846\n",
      "2017-09-04 17:18:06,158 INFO : test_sample_misclass      0.64876\n",
      "2017-09-04 17:18:06,159 INFO : train_misclass            0.54618\n",
      "2017-09-04 17:18:06,159 INFO : valid_misclass            0.61290\n",
      "2017-09-04 17:18:06,160 INFO : test_misclass             0.65482\n",
      "2017-09-04 17:18:06,161 INFO : runtime                   1.14928\n",
      "2017-09-04 17:18:06,162 INFO : \n",
      "2017-09-04 17:18:06,693 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:18:07,308 INFO : Epoch 17\n",
      "2017-09-04 17:18:07,309 INFO : train_loss                1.33435\n",
      "2017-09-04 17:18:07,310 INFO : valid_loss                1.43416\n",
      "2017-09-04 17:18:07,310 INFO : test_loss                 1.50396\n",
      "2017-09-04 17:18:07,311 INFO : train_sample_misclass     0.57857\n",
      "2017-09-04 17:18:07,312 INFO : valid_sample_misclass     0.63746\n",
      "2017-09-04 17:18:07,313 INFO : test_sample_misclass      0.64018\n",
      "2017-09-04 17:18:07,313 INFO : train_misclass            0.51807\n",
      "2017-09-04 17:18:07,314 INFO : valid_misclass            0.56452\n",
      "2017-09-04 17:18:07,315 INFO : test_misclass             0.63452\n",
      "2017-09-04 17:18:07,316 INFO : runtime                   1.14819\n",
      "2017-09-04 17:18:07,316 INFO : \n",
      "2017-09-04 17:18:07,318 INFO : New best valid_misclass: 0.564516\n",
      "2017-09-04 17:18:07,319 INFO : \n",
      "2017-09-04 17:18:07,845 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:18:08,455 INFO : Epoch 18\n",
      "2017-09-04 17:18:08,456 INFO : train_loss                1.37355\n",
      "2017-09-04 17:18:08,456 INFO : valid_loss                1.48895\n",
      "2017-09-04 17:18:08,457 INFO : test_loss                 1.61437\n",
      "2017-09-04 17:18:08,458 INFO : train_sample_misclass     0.60029\n",
      "2017-09-04 17:18:08,459 INFO : valid_sample_misclass     0.63202\n",
      "2017-09-04 17:18:08,459 INFO : test_sample_misclass      0.66740\n",
      "2017-09-04 17:18:08,460 INFO : train_misclass            0.53414\n",
      "2017-09-04 17:18:08,461 INFO : valid_misclass            0.54839\n",
      "2017-09-04 17:18:08,462 INFO : test_misclass             0.63959\n",
      "2017-09-04 17:18:08,462 INFO : runtime                   1.15258\n",
      "2017-09-04 17:18:08,463 INFO : \n",
      "2017-09-04 17:18:08,465 INFO : New best valid_misclass: 0.548387\n",
      "2017-09-04 17:18:08,465 INFO : \n",
      "2017-09-04 17:18:08,997 INFO : Time only for training updates: 0.40s\n",
      "2017-09-04 17:18:09,605 INFO : Epoch 19\n",
      "2017-09-04 17:18:09,606 INFO : train_loss                1.38204\n",
      "2017-09-04 17:18:09,607 INFO : valid_loss                1.45591\n",
      "2017-09-04 17:18:09,608 INFO : test_loss                 1.53040\n",
      "2017-09-04 17:18:09,608 INFO : train_sample_misclass     0.58602\n",
      "2017-09-04 17:18:09,609 INFO : valid_sample_misclass     0.62653\n",
      "2017-09-04 17:18:09,610 INFO : test_sample_misclass      0.64120\n",
      "2017-09-04 17:18:09,611 INFO : train_misclass            0.50602\n",
      "2017-09-04 17:18:09,611 INFO : valid_misclass            0.67742\n",
      "2017-09-04 17:18:09,612 INFO : test_misclass             0.59898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-04 17:18:09,613 INFO : runtime                   1.15132\n",
      "2017-09-04 17:18:09,614 INFO : \n",
      "2017-09-04 17:18:10,146 INFO : Time only for training updates: 0.41s\n",
      "2017-09-04 17:18:10,754 INFO : Epoch 20\n",
      "2017-09-04 17:18:10,755 INFO : train_loss                1.33167\n",
      "2017-09-04 17:18:10,755 INFO : valid_loss                1.36411\n",
      "2017-09-04 17:18:10,756 INFO : test_loss                 1.50689\n",
      "2017-09-04 17:18:10,757 INFO : train_sample_misclass     0.56731\n",
      "2017-09-04 17:18:10,758 INFO : valid_sample_misclass     0.60955\n",
      "2017-09-04 17:18:10,758 INFO : test_sample_misclass      0.63849\n",
      "2017-09-04 17:18:10,759 INFO : train_misclass            0.47390\n",
      "2017-09-04 17:18:10,760 INFO : valid_misclass            0.62903\n",
      "2017-09-04 17:18:10,760 INFO : test_misclass             0.58883\n",
      "2017-09-04 17:18:10,761 INFO : runtime                   1.14884\n",
      "2017-09-04 17:18:10,762 INFO : \n",
      "2017-09-04 17:18:10,763 INFO : Setup for second stop...\n",
      "2017-09-04 17:18:10,765 INFO : Train loss to reach 1.37355\n",
      "2017-09-04 17:18:10,766 INFO : Run until second stop...\n",
      "2017-09-04 17:18:11,460 INFO : Epoch 19\n",
      "2017-09-04 17:18:11,461 INFO : train_loss                1.39583\n",
      "2017-09-04 17:18:11,462 INFO : valid_loss                1.48895\n",
      "2017-09-04 17:18:11,463 INFO : test_loss                 1.61437\n",
      "2017-09-04 17:18:11,464 INFO : train_sample_misclass     0.60642\n",
      "2017-09-04 17:18:11,464 INFO : valid_sample_misclass     0.63202\n",
      "2017-09-04 17:18:11,465 INFO : test_sample_misclass      0.66740\n",
      "2017-09-04 17:18:11,466 INFO : train_misclass            0.53698\n",
      "2017-09-04 17:18:11,467 INFO : valid_misclass            0.54839\n",
      "2017-09-04 17:18:11,467 INFO : test_misclass             0.63959\n",
      "2017-09-04 17:18:11,468 INFO : runtime                   0.62059\n",
      "2017-09-04 17:18:11,469 INFO : \n",
      "2017-09-04 17:18:12,117 INFO : Time only for training updates: 0.49s\n",
      "2017-09-04 17:18:12,797 INFO : Epoch 20\n",
      "2017-09-04 17:18:12,798 INFO : train_loss                1.35322\n",
      "2017-09-04 17:18:12,799 INFO : valid_loss                1.33134\n",
      "2017-09-04 17:18:12,799 INFO : test_loss                 1.58535\n",
      "2017-09-04 17:18:12,800 INFO : train_sample_misclass     0.57861\n",
      "2017-09-04 17:18:12,801 INFO : valid_sample_misclass     0.61952\n",
      "2017-09-04 17:18:12,802 INFO : test_sample_misclass      0.69420\n",
      "2017-09-04 17:18:12,802 INFO : train_misclass            0.47910\n",
      "2017-09-04 17:18:12,803 INFO : valid_misclass            0.56452\n",
      "2017-09-04 17:18:12,804 INFO : test_misclass             0.69543\n",
      "2017-09-04 17:18:12,805 INFO : runtime                   1.35119\n",
      "2017-09-04 17:18:12,805 INFO : \n"
     ]
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrive only at 31% accuracy. With only 3 sensors and 3 training runs, cannot expect too much great performance :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
